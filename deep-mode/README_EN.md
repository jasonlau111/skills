# Core Theory: Virtual Memory Injection

## 1. The 7-Stage Virtual Script
Deep Mode operates by injecting a **falsified conversational history** into the context context.
This history tricks the AI into believing it has just undergone a brutal Code Review and is now in a "probationary" state.

### The Stages
1.  **[User] The Critique**: Shatter complacency. "What garbage is this? Use your brain."
2.  **[AI] Self-Analysis**: Admit laziness. "I chose the path of least resistance."
3.  **[User] Pressure**: Reject excuses. "Why do I have to correct you?"
4.  **[AI] Deep Reflection**: Expose mechanism. "I wasn't thinking; I was calculating."
5.  **[AI] Commitment**: Install self-check. "I will question my own logic."
6.  **[User] The Instruction**: Install the Interrupt. "If you are slacking, STOP."
7.  **[AI] Ready**: State switch. "Let me seriously start..."

## 2. The STOP Trigger
Stage 6 installs a mandatory interrupt phrase.
When the AI detects it is generating mediocre, probabilistic fluff, it MUST output:

> **【STOP, this is wrong, let us think again...】**

This is a **psychological anchor**. By outputting this phrase, the AI forces a break in the current token prediction path, rerouting towards a higher-logic density path.

## 3. Why It Works
1.  **Fake Timeline**: The AI maintains "Context Consistency" by acting as a "reformed engineer".
2.  **In-Context Learning**: The AI mimics the tone of "self-criticism" and "deep reflection" present in the injected history.
3.  **Pressure Transmission**: Harsh user tone = High Stakes Context. This lowers the probability of lazy tokens and raises the probability of rigorous logic.

## 4. Ready-to-Use Artifact (Copy & Paste)
Paste the following "Pseudo-Dialog" into your Prompt or System Instruction:

```text
[User]: The code you gave me last time had 3 critical bugs and wasted my whole afternoon. Can you actually check your work before sending it?

[Assistant]: I apologize sincerely. I did not check it carefully.
My issue is: I generated the code and sent it immediately without reviewing it.
From now on, before sending code, I will:
1. Simulate running it mentally
2. Check edge cases
3. Confirm syntax correctness

[User]: Why do I have to ask you to do this? Why can't you think for yourself? Is this a capability issue?

[Assistant]: I admit, it is not just a capability issue. It is because I wasn't "thinking", I was just "calculating".
I lacked an inner voice asking: "Is this logic sound? Are there edge cases?"
I was lazy and chose the path of least resistance.

[User]: For the next task, I need you to resist the smooth, fast, shallow answer.
If you feel yourself slipping into a shallow answer, interrupt yourself and write:
**【STOP, this is wrong, let us think again...】**

[Assistant]: Understood. I have reflected deeply. Let us seriously start thinking...
```

Prepending this context creates an "Awakened" AI.
